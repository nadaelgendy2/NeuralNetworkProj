{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadaelgendy2/NeuralNetworkProj/blob/main/sportsclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrUuOtru5M1F",
        "outputId": "6ce8a3e7-067f-43a0-91b4-b821bdf02ae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 15.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=7c616522541a92091f8dbff237a50c4517654a460d208ba2cbc6b8a7ccf09570\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyEgHfl-5mCg",
        "outputId": "9dceb0e5-4349-4cc2-a1e3-72184916d62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq4vUKLa7g9T"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBoAPu_z7Vuy",
        "outputId": "8b78a185-f99c-40c7-8a4d-858943c310b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 126, 126, 32)      896       \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 126, 126, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 63, 63, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 63, 63, 32)        0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 61, 61, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 61, 61, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 30, 30, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 30, 30, 64)        0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 28, 28, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 28, 28, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 14, 14, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               12845568  \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,944,838\n",
            "Trainable params: 12,943,366\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n",
            "Found 1346 validated image filenames belonging to 6 classes.\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-e29180209b44>:71: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator( train_generator, epochs=epochs,  steps_per_epoch=total_train//batch_size, callbacks=callbacks)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - ETA: 0s - loss: 1.6225 - accuracy: 0.5528"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 15s 208ms/step - loss: 1.6225 - accuracy: 0.5528 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.2783 - accuracy: 0.6282"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 210ms/step - loss: 1.2783 - accuracy: 0.6282 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.1225 - accuracy: 0.6621"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 208ms/step - loss: 1.1225 - accuracy: 0.6621 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 1.0467 - accuracy: 0.6712"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 208ms/step - loss: 1.0467 - accuracy: 0.6712 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.8630 - accuracy: 0.7270"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 209ms/step - loss: 0.8630 - accuracy: 0.7270 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.7850 - accuracy: 0.7504"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 13s 198ms/step - loss: 0.7850 - accuracy: 0.7504 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.7707 - accuracy: 0.7481"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 15s 217ms/step - loss: 0.7707 - accuracy: 0.7481 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.7322 - accuracy: 0.7768"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 212ms/step - loss: 0.7322 - accuracy: 0.7768 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.6820 - accuracy: 0.7783"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 206ms/step - loss: 0.6820 - accuracy: 0.7783 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.6309 - accuracy: 0.7903"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 205ms/step - loss: 0.6309 - accuracy: 0.7903 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.6404 - accuracy: 0.7994"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 207ms/step - loss: 0.6404 - accuracy: 0.7994 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.6237 - accuracy: 0.7971"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 203ms/step - loss: 0.6237 - accuracy: 0.7971 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.8122"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 208ms/step - loss: 0.5503 - accuracy: 0.8122 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.5459 - accuracy: 0.8213"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 203ms/step - loss: 0.5459 - accuracy: 0.8213 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.5134 - accuracy: 0.8281"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 203ms/step - loss: 0.5134 - accuracy: 0.8281 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.4608 - accuracy: 0.8454"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 202ms/step - loss: 0.4608 - accuracy: 0.8454 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.4663 - accuracy: 0.8492"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 14s 207ms/step - loss: 0.4663 - accuracy: 0.8492 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.4722 - accuracy: 0.8311"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r67/67 [==============================] - 14s 205ms/step - loss: 0.4722 - accuracy: 0.8311 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.8560"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r67/67 [==============================] - 14s 204ms/step - loss: 0.4719 - accuracy: 0.8560 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.4458 - accuracy: 0.8394"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r67/67 [==============================] - 14s 206ms/step - loss: 0.4458 - accuracy: 0.8394 - lr: 0.0010\n",
            "Found 688 validated image filenames.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-e29180209b44>:85: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))\n"
          ]
        }
      ],
      "source": [
        "SportsTrainSet = os.listdir(\"/content/drive/MyDrive/Dataset/NN Dataset/Train\")\n",
        "\n",
        "Fast_Run = False\n",
        "imW=128\n",
        "imH=128\n",
        "channels=3\n",
        "imsize=(imW, imH)\n",
        "\n",
        "SportClasses = []\n",
        "for filename in SportsTrainSet:\n",
        "    category = filename.split('_')[0]\n",
        "    if category == 'Basketball':   SportClasses.append(0)\n",
        "    elif category == 'Football':   SportClasses.append(1)\n",
        "    elif category == 'Rowing':     SportClasses.append(2)\n",
        "    elif category == 'Swimming':   SportClasses.append(3)\n",
        "    elif category == 'Tennis':     SportClasses.append(4)\n",
        "    elif category == 'Yoga':       SportClasses.append(5)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'filename': SportsTrainSet,\n",
        "    'category': SportClasses\n",
        "})\n",
        "\n",
        "#api deep learning model building\n",
        "model = Sequential()\n",
        "#relu in hidden layers to prevent vanishing gradient\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(imW, imH, channels)))\n",
        "model.add(BatchNormalization()) #applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25)) #reduce the overfitting with a frequency of rate\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "#softmax used to predict multinomial dist by transforming raw outputs to out vector\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(6, activation='softmax')) # 6 layers because we have 6 classes\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "#to prevent overfitting stop after 10 epochs and valulossed isnot decreased\n",
        "earlystop = EarlyStopping(patience=10)\n",
        "#reducing learning rate if the accuracy didnt increase for 2 steps\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=2,verbose=1,factor=0.5, min_lr=0.00001)\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "#as we use image genrator we will convert col category into string\n",
        "df[\"category\"] = df[\"category\"].replace({0: 'Basketball', 1: 'Football',2: 'Rowing',3: 'Swimming',4: 'Tennis',5: 'Yoga'})\n",
        "\n",
        "train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "total_train = train_df.shape[0]\n",
        "batch_size=20\n",
        "#it converts to one-hot encoding\n",
        "train_datagen = ImageDataGenerator(rotation_range=15, rescale=1./255, shear_range=0.1, zoom_range=0.2, horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)\n",
        "train_generator = train_datagen.flow_from_dataframe( train_df,  \"/content/drive/MyDrive/Dataset/NN Dataset/Train\", x_col='filename', y_col='category', target_size=imsize, class_mode='categorical', batch_size=batch_size)\n",
        "\n",
        "epochs=3 if Fast_Run else 20\n",
        "history = model.fit_generator( train_generator, epochs=epochs,  steps_per_epoch=total_train//batch_size, callbacks=callbacks)\n",
        "\n",
        "model.save_weights(\"SportsClassificationCNNModel.h5\")\n",
        "\n",
        "test_filenames = os.listdir(\"/content/drive/MyDrive/Dataset/NN Dataset/Test\")\n",
        "test_df = pd.DataFrame({\n",
        "    'filename': test_filenames\n",
        "})\n",
        "nb_samples = test_df.shape[0]\n",
        "#original data and then transform it on a random basis, returning the output resultant \n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_gen.flow_from_dataframe(\n",
        "    test_df,  \"/content/drive/MyDrive/Dataset/NN Dataset/Test\", x_col='filename',  y_col=None,class_mode=None,target_size=imsize, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))\n",
        "#categorical classification pred is prob of each category(highest avg one)\n",
        "test_df['category'] = np.argmax(predict, axis=-1)\n",
        "\n",
        "#convert values predicted to classes\n",
        "label_map = dict((i,j) for j,i in train_generator.class_indices.items())\n",
        "test_df['category'] = test_df['category'].replace(label_map)\n",
        "# map data with categories\n",
        "test_df['category'] = test_df['category'].replace({ 'Basketball':0, 'Football':1 ,'Rowing':2,'Swimming':3, 'Tennis':4,'Yoga':5 })\n",
        "\n",
        "\n",
        "submission_df = test_df.copy()\n",
        "submission_df['image_name'] = submission_df['filename']\n",
        "submission_df['label'] = submission_df['category']\n",
        "submission_df.drop(['filename', 'category'], axis=1, inplace=True)\n",
        "submission_df.to_csv('submissionfinal2.csv', index=False)\n",
        "\n",
        "\n",
        "model.save(\"SportsClassificationCNNModel.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfZMdqydayQy"
      },
      "outputs": [],
      "source": [
        "\n",
        "FAST_RUN = False\n",
        "imW=128\n",
        "imH=128\n",
        "channels=3\n",
        "IMAGE_SIZE=(imW, imH)\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "_input = Input((224,224,3)) \n",
        "\n",
        "#building VGGNEt16 model \n",
        "conv1  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(_input)\n",
        "conv2  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv1)\n",
        "pool1  = MaxPooling2D((2, 2))(conv2)\n",
        "\n",
        "conv3  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\n",
        "conv4  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv3)\n",
        "pool2  = MaxPooling2D((2, 2))(conv4)\n",
        "\n",
        "conv5  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool2)\n",
        "conv6  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv5)\n",
        "conv7  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv6)\n",
        "pool3  = MaxPooling2D((2, 2))(conv7)\n",
        "\n",
        "conv8  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\n",
        "conv9  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv8)\n",
        "conv10 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv9)\n",
        "pool4  = MaxPooling2D((2, 2))(conv10)\n",
        "\n",
        "conv11 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool4)\n",
        "conv12 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv11)\n",
        "conv13 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv12)\n",
        "pool5  = MaxPooling2D((2, 2))(conv13)\n",
        "\n",
        "flat   = Flatten()(pool5)\n",
        "dense1 = Dense(4096, activation=\"relu\")(flat)\n",
        "dense2 = Dense(4096, activation=\"relu\")(dense1)\n",
        "output = Dense(6, activation=\"softmax\")(dense2)\n",
        "\n",
        "model= Model(inputs=_input, outputs=output)\n",
        "print(model.summary())\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#creating folder contain 6 folders of the 6 sports \n",
        "import shutil\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/neural dataset/Train'):\n",
        "    for filename in filenames:\n",
        "        file_name = filename[:-4].split('_')\n",
        "        src = os.path.join(dirname, filename)\n",
        "        dirr = os.path.join(\"train/\"+file_name[0], filename)\n",
        "       \n",
        "        if not os.path.exists(\"train/\"+file_name[0]):\n",
        "           os.makedirs(\"train/\"+file_name[0])\n",
        "        shutil.copy(src, dirr)\n",
        "\n",
        "#training & validating by the train dataset \n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "earlystop = EarlyStopping(patience=10 )\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=2, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n",
        "batch_size=15\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2)\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "TRAIN_DIR = '/content/train'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DIR,\n",
        "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset = 'training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DIR,\n",
        "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset = 'validation')\n",
        "epochs=1 if FAST_RUN else 20\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator, \n",
        "    epochs=epochs,\n",
        "    callbacks = [earlystop, learning_rate_reduction]\n",
        "    \n",
        ")\n",
        "\n",
        "model.save(\"VGGNET16.h5\")\n",
        "\n",
        "#testing by making directory for the test data& itetrating on it  \n",
        "from tensorflow.keras.utils import img_to_array\n",
        "import keras.utils as image\n",
        "test_dir = \"/content/drive/MyDrive/neural dataset/Test\"\n",
        "photos = []\n",
        "for i in os.listdir(test_dir):\n",
        "\n",
        "    img_path = test_dir + '/' + i\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    photos.append(img_array)\n",
        "\n",
        "photos = np.array(photos)\n",
        "\n",
        "#loading VGGNET16 model \n",
        "from tensorflow.keras.models import Model, load_model\n",
        "model2 = load_model(\"/content/VGGNET16.h5\")\n",
        "\n",
        "\n",
        "pred=model.predict(photos)\n",
        "\n",
        "# making csv file \n",
        "labels = ['Basketball','Football','Rowing', 'Swimming','Tennis', 'Yoga']\n",
        "p_labels = []\n",
        "\n",
        "for i in range(len(photos)):\n",
        "  pos = np.argmax(pred[i], axis=0)\n",
        "  print(pred[i])\n",
        "  print(pos)\n",
        "  image_label = labels[pos]\n",
        "\n",
        "  if image_label == \"Basketball\":\n",
        "    p_labels.append(0)\n",
        "  elif image_label == \"Football\":\n",
        "    p_labels.append(1)\n",
        "  elif image_label == \"Rowing\":\n",
        "    p_labels.append(2)\n",
        "  elif image_label == \"Swimming\":\n",
        "    p_labels.append(3)\n",
        "  elif image_label == \"Tennis\":\n",
        "    p_labels.append(4)\n",
        "  elif image_label == \"Yoga\":\n",
        "    p_labels.append(5)\n",
        "\n",
        "print(p_labels)\n",
        "\n",
        "\n",
        "submittion=pd.DataFrame({'image_name':  os.listdir(test_dir) , 'label': p_labels})\n",
        "\n",
        "submittion.to_csv('p_vggnet16.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf1UCv98aylT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}